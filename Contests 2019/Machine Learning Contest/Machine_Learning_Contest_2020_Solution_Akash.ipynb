{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Machine Learning Contest-2020 Solution_Akash.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwItnkLzeW_s",
        "colab_type": "text"
      },
      "source": [
        "# Solution of Machine Learning Contest-2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-FJYWzeW_0",
        "colab_type": "text"
      },
      "source": [
        "*Solution for Machine Learning Contest-2020*\n",
        "\n",
        "*Source of Dataset : https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrDTQJzMeW_2",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YEYWXLCeW_4",
        "colab_type": "code",
        "outputId": "1af3cd50-dfea-43f3-83c0-7138f291bf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:90% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqCi96LteXAB",
        "colab_type": "text"
      },
      "source": [
        "## Importing and Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6HpsceOeXAD",
        "colab_type": "text"
      },
      "source": [
        "### Importing and Viewing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXV8hevneXAF",
        "colab_type": "text"
      },
      "source": [
        "*Importing Data from csv file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL2909UeXAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('LinearReg.csv',sep=';')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a68l1iSyeXAL",
        "colab_type": "text"
      },
      "source": [
        "*Printing Data and its Statistics*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pGGA6CyaeXAM",
        "colab_type": "code",
        "outputId": "fcf6a744-8996-4129-92b9-1c8cc5728f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "df = df.reindex(np.random.permutation(df.index))\t\t\t#Shuffle\n",
        "print (df)\n",
        "print (df.describe())\t\t\t\t\t#Gives statitics of the data"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
            "1228            5.1             0.420         0.00  ...       0.73     13.6        7\n",
            "286            12.0             0.450         0.55  ...       0.76     10.3        6\n",
            "130             8.0             0.745         0.56  ...       0.66      9.4        5\n",
            "1040            7.4             0.965         0.00  ...       0.67     10.2        5\n",
            "223             8.6             0.645         0.25  ...       0.60     10.0        6\n",
            "...             ...               ...          ...  ...        ...      ...      ...\n",
            "1596            6.3             0.510         0.13  ...       0.75     11.0        6\n",
            "768             7.1             0.590         0.02  ...       0.53      9.7        6\n",
            "258             7.7             0.410         0.76  ...       1.26      9.4        5\n",
            "942            10.1             0.430         0.40  ...       0.64     10.0        7\n",
            "1526            6.8             0.470         0.08  ...       0.65      9.6        6\n",
            "\n",
            "[1599 rows x 12 columns]\n",
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    1599.000000       1599.000000  ...  1599.000000  1599.000000\n",
            "mean        8.319637          0.527821  ...    10.422983     5.636023\n",
            "std         1.741096          0.179060  ...     1.065668     0.807569\n",
            "min         4.600000          0.120000  ...     8.400000     3.000000\n",
            "25%         7.100000          0.390000  ...     9.500000     5.000000\n",
            "50%         7.900000          0.520000  ...    10.200000     6.000000\n",
            "75%         9.200000          0.640000  ...    11.100000     6.000000\n",
            "max        15.900000          1.580000  ...    14.900000     8.000000\n",
            "\n",
            "[8 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PswCW0sHeXAR",
        "colab_type": "code",
        "outputId": "ff1bcc3f-5ff2-4fc1-9cf4-5d24a87cfa95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "X = df.loc[:, df.columns != \"quality\"] # Defining Input and Output variables\n",
        "y = df[\"quality\"]\n",
        "print (X.columns.values)\n",
        "print (y)"
      ],
      "execution_count": 441,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fixed acidity' 'volatile acidity' 'citric acid' 'residual sugar'\n",
            " 'chlorides' 'free sulfur dioxide' 'total sulfur dioxide' 'density' 'pH'\n",
            " 'sulphates' 'alcohol']\n",
            "1228    7\n",
            "286     6\n",
            "130     5\n",
            "1040    5\n",
            "223     6\n",
            "       ..\n",
            "1596    6\n",
            "768     6\n",
            "258     5\n",
            "942     7\n",
            "1526    6\n",
            "Name: quality, Length: 1599, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgr8PGmreXAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#converting pandas Dataframe to Numpy Arrays\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1EqlCFkeXAZ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWkbpECHeXAa",
        "colab_type": "text"
      },
      "source": [
        "*Preprocessing Data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ysSmmqhnWU5",
        "colab_type": "text"
      },
      "source": [
        "#### Feature Normalisation\n",
        "$$X=\\frac{X-\\mu}{\\sigma}$$\n",
        "First, for each feature dimension, compute the mean of the feature\n",
        "and subtract it from the dataset, storing the mean value in mu ($\\mu$). \n",
        "Next, compute the  standard deviation of each feature and divide\n",
        "each feature by it's standard deviation, storing the standard deviation \n",
        "in sigma ($\\sigma$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsglkvr6eXAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Feature_Normalize(X):\n",
        "    \"\"\"\n",
        "    Normalizes the features in X. returns a normalized version of X.\n",
        "    \"\"\"\n",
        "    mu = np.mean(X, axis = 0)       #computing mean\n",
        "    sigma = np.std(X, axis = 0)\n",
        "    X_norm = (X - mu) / sigma       #Feature Normalisation\n",
        "    return X_norm, mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEPBSPv4eXAf",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the Data for Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHw-fn_ReXAg",
        "colab_type": "text"
      },
      "source": [
        "*Splitting Data to Train and Test sets to train and evaluate the Model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVX8sgHDeXAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)#Train-Test Split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZlLQHNseXAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train= Feature_Normalize(X_train)[0]\n",
        "X_test = Feature_Normalize(X_test)[0]\n",
        "X_train = np.c_[np.ones(X_train.shape[0]), X_train] #Add a column of ones\n",
        "X_test = np.c_[np.ones(X_test.shape[0]), X_test] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IpFvHhUdeXAo",
        "colab_type": "code",
        "outputId": "c87c5eff-e842-4d19-9f41-b80c508af0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print (\"Shape of Training Set is\",X_train.shape)\n",
        "print (\"Shape of Test Set is\",X_test.shape)\n",
        "print (\"Shape of Training Set is\",y_train.shape)\n",
        "print (\"Shape of Test Set is\",y_test.shape)"
      ],
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training Set is (1279, 12)\n",
            "Shape of Test Set is (320, 12)\n",
            "Shape of Training Set is (1279,)\n",
            "Shape of Test Set is (320,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xMC5Iqir9d",
        "colab_type": "text"
      },
      "source": [
        "## Objective, Cost function and Gradient Descent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuH3dR64iKAh",
        "colab_type": "text"
      },
      "source": [
        "The objective of linear regression is to minimize the cost function\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 +\\frac{\\lambda}{2m}\\sum_{j=1}^n {\\theta_j}^2$$\n",
        "where the hypothesis $h_\\theta(x)$ is given by the linear model$$ h_\\theta(x) = \\theta^Tx = \\sum_{i=0}^n \\theta_ix_i$$\n",
        "\n",
        "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update\n",
        "\n",
        "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\quad \\forall j>=1  \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
        "where, $$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j$$\n",
        "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$).\n",
        "\n",
        "**Implementation Note:** We store each example as a row in the the $X$ matrix in Python `numpy`. To take into account the intercept term ($\\theta_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $\\theta_0$ as simply another 'feature'.\n",
        "\n",
        "We are ignoring the Regularisation term in this approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa39onkweXAs",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uslycZpaeXAt",
        "colab_type": "text"
      },
      "source": [
        "*Compute Cost function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0esbP8k-eXAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CostFunction(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute cost for linear regression with multiple variables.\n",
        "    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
        "    \"\"\"\n",
        "    m = y.shape[0] # number of training examples\n",
        "    J = 0\n",
        "    h = X.dot(theta.T)\n",
        "    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))#definition of cost function\n",
        "    return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po0zM4LgeXAy",
        "colab_type": "text"
      },
      "source": [
        "*Gradient Descent*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6_94OMeeXAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GradientDescent(X, y, theta, lr):\n",
        "    \"\"\"\n",
        "    Performs gradient descent to learn theta.\n",
        "    Updates theta by taking num_iters gradient steps with learning rate alpha.\n",
        "    \"\"\"\n",
        "    # Initialize some useful values\n",
        "    m = y.shape[0] # number of training examples\n",
        "    theta = theta - (lr / m) * (np.dot(X, theta) - y).dot(X)#Gradient descent algorithm\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HdLdoKceXA2",
        "colab_type": "text"
      },
      "source": [
        "*Function to Calculate Validation Loss*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWdWT5QVeXA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Validation_Loss_Function(y_pred,y):\n",
        "    \"\"\"\n",
        "    Function to calculate Validation Loss\n",
        "    \"\"\"\n",
        "    m = y.shape[0]\n",
        "    loss = np.square(y_pred-y)/(m)\n",
        "    loss = np.sum(loss)\n",
        "    loss = np.sqrt(loss)\n",
        "    \n",
        "    return loss   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtCSvFNteXA6",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxgLi9KeXA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Training_and_Validation(X_train,y_train,X_test,y_test,lr=0.3,epochs=10):\n",
        "    \"\"\"\n",
        "    Training Linear Regression Model\n",
        "    \"\"\"\n",
        "    s = X_train.shape[1]\n",
        "    theta = np.zeros(s)\n",
        "    Training_Loss = []\n",
        "    Validation_Loss = []\n",
        "    for i in range(epochs):\n",
        "        theta = GradientDescent(X_train, y_train, theta, lr)\n",
        "        Training_Loss.append(CostFunction(X_train,y_train,theta))\n",
        "        y_pred = np.dot(X_test, theta)\n",
        "        Validation_Loss.append(Validation_Loss_Function(y_pred,y_test))\n",
        "        if (i%50 == 0):\n",
        "            print (\"Epoch : {}  |  Training Loss : {}  |  Validation Loss : {}\".format(i, round(Training_Loss[i],6), round(Validation_Loss[i],5)))\n",
        "    plt.plot(Training_Loss,'r')\n",
        "    plt.plot(Validation_Loss,'y')\n",
        "    plt.show()\n",
        "      \n",
        "    print (\"Final Training Loss is equal to \",Training_Loss[-1])\n",
        "    print (\"Final Validation Loss is equal to \",Validation_Loss[-1])\n",
        "    \n",
        "    return theta  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P75LdxRVeXA-",
        "colab_type": "code",
        "outputId": "9d21be64-c112-451b-bba6-1c826d99422e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "start= time.time()\n",
        "theta = Training_and_Validation(X_train,y_train,X_test,y_test)\n",
        "end= time.time()\n",
        "print ('Total time taken for training is:',end-start,'sec')\n",
        "print(theta)"
      ],
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0  |  Training Loss : 8.023372  |  Validation Loss : 4.00261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Final Training Loss is equal to  0.21915549604565593\n",
            "Final Validation Loss is equal to  0.6841645056031475\n",
            "Total time taken for training is: 0.23940110206604004 sec\n",
            "[ 5.47797916  0.0542222  -0.18687125  0.02988732  0.02564634 -0.08655637\n",
            "  0.0214673  -0.09390555 -0.07598728 -0.01446124  0.15006749  0.25710589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLMzQiNYw-40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalised_eqns(X,y):\n",
        "    # Computes the closed-form solution to linear regression using the normal equations.\n",
        "    theta = np.zeros(X_train.shape[1])\n",
        "    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)\n",
        "    return theta\n",
        "theta_normal= Normalised_eqns(X_train,y_train)\n",
        "print(\"Theta computed by Normal Eqns is :\" , theta_normal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy1Iad_xx_14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_array = [1,7.4,0.7,0,1.9,0.076,11,34,0.9978,3.51,0.56,9.4]\n",
        "X_array = np.asarray(X_array)\n",
        "X_array[1:11] = (X_array[1:11] - np.mean(X_array[1:11])) /(np.std(X_array[1:11]))\n",
        "quality_normalised_eqns = np.dot(X_array,Normalised_eqns(X_train,y_train) ) \n",
        "quality_by_gradient_descent=np.dot(X_array,theta)\n",
        "print('quality by normalised_eqns :', quality_normalised_eqns)\n",
        "print('quality by Gradient descent:', quality_by_gradient_descent)\n",
        "closeness=np.allclose(theta_normal,theta,rtol=1e-2, atol=1e-2)\n",
        "print(closeness)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}